{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import numpy as np \r\n",
    "import pandas as pd \r\n",
    "import matplotlib.pyplot as plt \r\n",
    "import random\r\n",
    "import cv2\r\n",
    "\r\n",
    "import torch\r\n",
    "import torchvision\r\n",
    "import torch.nn as nn\r\n",
    "import torchvision.transforms as transforms\r\n",
    "\r\n",
    "from torch.autograd import Function # khi gọi class, nó tự động tính hàm foward()\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class Base(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Base, self).__init__()\r\n",
    "\r\n",
    "        self.layer1 = nn.Sequential(\r\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), # (N, 64, 300, 300)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), # (N, 64, 300, 300)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # (N, 64, 150, 150)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer2 = nn.Sequential(\r\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), #(N, 128, 150, 150)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), # (N, 128, 150, 150)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # (N, 128, 75, 75)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer3 = nn.Sequential(\r\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), # (N, 256, 75, 75)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), # (N, 256, 75, 75)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), # (N, 256, 75, 75)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True) # (N, 256, 38, 38)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer4 = nn.Sequential(\r\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1), # (N, 512, 38, 38)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (N, 512, 38, 38)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (N, 512, 38, 38)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # (N, 512, 19, 19)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer5 = nn.Sequential(\r\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (N, 512, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (N, 512, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (N, 512, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1) # (N, 512, 19, 19)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer6 = nn.Sequential(\r\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6), # (N, 1024, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "        \r\n",
    "        self.layer7 = nn.Sequential(\r\n",
    "            nn.Conv2d(1024, 1024, kernel_size=1), # (N, 1024, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "\r\n",
    "        self.load_pretrained_params()\r\n",
    "\r\n",
    "    def forward(self, input):\r\n",
    "        #Base\r\n",
    "        out = self.layer1(input)\r\n",
    "        out = self.layer2(out)\r\n",
    "        out = self.layer3(out)\r\n",
    "        conv4_3_feat = out\r\n",
    "        \r\n",
    "        out = self.layer4(out)\r\n",
    "        out = self.layer5(out)\r\n",
    "        out = self.layer6(out)\r\n",
    "        out = self.layer7(out)\r\n",
    "        conv7_feat = out\r\n",
    "\r\n",
    "        return conv4_3_feat, conv7_feat \r\n",
    "\r\n",
    "    def load_pretrained_params(self):\r\n",
    "\r\n",
    "        state_dict = self.state_dict()\r\n",
    "        params_keys = list(state_dict.keys())\r\n",
    "\r\n",
    "        vgg = torchvision.models.vgg16(pretrained=True)\r\n",
    "            \r\n",
    "        pretrained_state_dict = vgg.state_dict()\r\n",
    "        pretrained_params_keys = list(pretrained_state_dict.keys())\r\n",
    "\r\n",
    "        for i, key in enumerate(params_keys[:-4]):\r\n",
    "            state_dict[key] = pretrained_state_dict[pretrained_params_keys[i]]\r\n",
    "\r\n",
    "\r\n",
    "        #Convert fc6, fc7 to convolutional layers\r\n",
    "        w_fc6 = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)\r\n",
    "        b_fc6 = pretrained_state_dict['classifier.0.bias'] # (4096,)\r\n",
    "\r\n",
    "        w_fc7 = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)\r\n",
    "        b_fc7 = pretrained_state_dict['classifier.3.bias'] #(4096, )\r\n",
    "\r\n",
    "        # Subsample parameters of fc6, fc7\r\n",
    "        w_conv6 = torch.index_select(input=w_fc6, dim=0, index=torch.arange(0, 4096, step=4)) # (1024, 512, 7, 7)\r\n",
    "        w_conv6 = torch.index_select(input=w_conv6, dim=2, index=torch.arange(0, 7, step=3)) # (1024, 512, 3, 7)\r\n",
    "        w_conv6 = torch.index_select(input=w_conv6, dim=3, index=torch.arange(0, 7, step=3)) #(1024, 512, 3, 3)\r\n",
    "        \r\n",
    "        b_conv6 = torch.index_select(input=b_fc6, dim=0, index=torch.arange(0, 4096, step=4)) #(1024,)\r\n",
    "\r\n",
    "\r\n",
    "        w_conv7 = torch.index_select(input=w_fc7, dim=0, index=torch.arange(0, 4096, step=4)) #(1024, 4096, 1, 1)\r\n",
    "        w_conv7 = torch.index_select(input=w_conv7, dim=1, index=torch.arange(0, 4096, step=4)) #(1024, 1024, 1, 1)\r\n",
    "\r\n",
    "        b_conv7 = b_conv6 = torch.index_select(input=b_fc6, dim=0, index=torch.arange(0, 4096, step=4)) #(1024,)\r\n",
    "\r\n",
    "\r\n",
    "        state_dict['layer6.0.weight'] = w_conv6\r\n",
    "        state_dict['layer6.0.bias'] = b_conv6\r\n",
    "        state_dict['layer7.0.weight'] = w_conv7\r\n",
    "        state_dict['layer7.0.bias'] = b_conv7\r\n",
    "\r\n",
    "        self.load_state_dict(state_dict)\r\n",
    "\r\n",
    "        print('Loaded pretrained model VGG to Base.') \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class Extras(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Extras, self).__init__()\r\n",
    "        self.layer8 = nn.Sequential(\r\n",
    "            nn.Conv2d(1024, 256, kernel_size=1, padding=0), #(N, 256, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2), # (N, 512, 10, 10)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer9 = nn.Sequential(\r\n",
    "            nn.Conv2d(512, 128, kernel_size=1, padding=0), # (N, 128, 10, 10)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1), # (N, 256, 5, 5)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer10 = nn.Sequential(\r\n",
    "            nn.Conv2d(256, 128, kernel_size=1, padding=0), #(N, 128, 5, 5)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=0), # (N, 256, 3, 3)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "        \r\n",
    "        self.layer11 = nn.Sequential(\r\n",
    "            nn.Conv2d(256, 128, kernel_size=1, padding=0), #(N, 128, 3, 3)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=0), #(N, 256, 1, 1)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "\r\n",
    "        self.init_params()\r\n",
    "\r\n",
    "    def init_params(self):\r\n",
    "        for c in self.children():\r\n",
    "            if isinstance(c, nn.Conv2d):\r\n",
    "                nn.init.xavier_uniform_(c.weight)\r\n",
    "                nn.init.constant_(c.bias, 0.)\r\n",
    "        \r\n",
    "    def forward(self, input):\r\n",
    "        #Extras\r\n",
    "        out = self.layer8(input)\r\n",
    "        conv8_2_feat = out\r\n",
    "        out = self.layer9(out)\r\n",
    "        conv9_2_feat = out\r\n",
    "        out = self.layer10(out)\r\n",
    "        conv10_2_feat = out\r\n",
    "        conv11_2_feat = self.layer11(out)\r\n",
    "\r\n",
    "        return conv8_2_feat, conv9_2_feat, conv10_2_feat, conv11_2_feat\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class Predict(nn.Module):\r\n",
    "    def __init__(self, num_classes):\r\n",
    "        super(Predict, self).__init__()\r\n",
    "        self.num_classes = num_classes\r\n",
    "        num_boxes = {'conv4_3':4, 'conv7':6, 'conv8_2':6, 'conv9_2':6, 'conv10_2':4, 'conv11_2':4} #Number of default boxes for each feature\r\n",
    "\r\n",
    "        #Location\r\n",
    "        self.loc_conv4_3 = nn.Conv2d(256, num_boxes['conv4_3']*4, kernel_size=3, padding=1)\r\n",
    "        self.loc_conv7 = nn.Conv2d(1024, num_boxes['conv7']*4, kernel_size=3, padding=1)\r\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, num_boxes['conv8_2']*4, kernel_size=3, padding=1)\r\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, num_boxes['conv9_2']*4, kernel_size=3, padding=1)\r\n",
    "        self.loc_conv10_2 = nn.Conv2d(256, num_boxes['conv10_2']*4, kernel_size=3, padding=1)\r\n",
    "        self.loc_conv11_2 = nn.Conv2d(256, num_boxes['conv11_2']*4, kernel_size=3, padding=1)\r\n",
    "\r\n",
    "        #Classify\r\n",
    "        self.cl_conv4_3 = nn.Conv2d(256, num_boxes['conv4_3']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "        self.cl_conv7 = nn.Conv2d(1024, num_boxes['conv7']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, num_boxes['conv8_2']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, num_boxes['conv9_2']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "        self.cl_conv10_2 = nn.Conv2d(256, num_boxes['conv10_2']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "        self.cl_conv11_2 = nn.Conv2d(256, num_boxes['conv11_2']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "\r\n",
    "        self.init_params()\r\n",
    "\r\n",
    "    def init_params(self):\r\n",
    "        for c in self.children():\r\n",
    "            if isinstance(c, nn.Conv2d):\r\n",
    "                nn.init.xavier_uniform_(c.weight)\r\n",
    "                nn.init.constant_(c.bias, 0.)\r\n",
    "    \r\n",
    "    \r\n",
    "    '''If you just want to reshape tensors, use torch.reshape.\r\n",
    "       If you're also concerned about memory usage and want to ensure that the two tensors share the same data, use torch.view.'''\r\n",
    "       \r\n",
    "    def forward(self, conv4_3_feat, conv7_feat, conv8_2_feat, conv9_2_feat, conv10_2_feat, conv11_2_feat):\r\n",
    "        batch_size = conv4_3_feat.shape[0]\r\n",
    "\r\n",
    "        #Location\r\n",
    "        loc_conv4_3 = self.loc_conv4_3(conv4_3_feat) # (N, 16, 38, 38)\r\n",
    "        loc_conv4_3 = loc_conv4_3.permute(0, 2, 3, 1).contiguous() # (N, 38, 38, 16)\r\n",
    "        loc_conv4_3 = loc_conv4_3.view(batch_size, -1, 4) #(N, 5776, 4)\r\n",
    "        \r\n",
    "        loc_conv7 = self.loc_conv7(conv7_feat) #(N, 24, 19, 19)\r\n",
    "        loc_conv7 = loc_conv7.permute(0, 2, 3, 1).contiguous() #(N, 19, 19, 24)\r\n",
    "        loc_conv7 = loc_conv7.view(batch_size, -1, 4) #(N, 2166, 4)\r\n",
    "\r\n",
    "        loc_conv8_2 = self.loc_conv8_2(conv8_2_feat)\r\n",
    "        loc_conv8_2 = loc_conv8_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        loc_conv8_2 = loc_conv8_2.view(batch_size, -1, 4)\r\n",
    "        \r\n",
    "        loc_conv9_2 = self.loc_conv9_2(conv9_2_feat)\r\n",
    "        loc_conv9_2 = loc_conv9_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        loc_conv9_2 = loc_conv9_2.view(batch_size, -1, 4)\r\n",
    "\r\n",
    "        loc_conv10_2 = self.loc_conv10_2(conv10_2_feat)\r\n",
    "        loc_conv10_2 = loc_conv10_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        loc_conv10_2 = loc_conv10_2.view(batch_size, -1, 4)\r\n",
    "\r\n",
    "        loc_conv11_2 = self.loc_conv11_2(conv11_2_feat)\r\n",
    "        loc_conv11_2 = loc_conv11_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        loc_conv11_2 = loc_conv11_2.view(batch_size, -1, 4)\r\n",
    "\r\n",
    "        #Classification\r\n",
    "        cl_conv4_3 = self.cl_conv4_3(conv4_3_feat)  #(N, classes*4, 38, 38)\r\n",
    "        cl_conv4_3 = cl_conv4_3.permute(0, 2, 3, 1).contiguous() #(N, 38, 38, classes*4)\r\n",
    "        cl_conv4_3 = cl_conv4_3.view(batch_size, -1, self.num_classes) # (N, 5776, classes)\r\n",
    "\r\n",
    "        cl_conv7 = self.cl_conv7(conv7_feat) #(N, classes*6, 19, 19)\r\n",
    "        cl_conv7 = cl_conv7.permute(0, 2, 3, 1).contiguous() # (N, 19, 19, classes*6)\r\n",
    "        cl_conv7 = cl_conv7.view(batch_size, -1, self.num_classes) # (N, 2166, classes)\r\n",
    "\r\n",
    "        cl_conv8_2 = self.cl_conv8_2(conv8_2_feat)\r\n",
    "        cl_conv8_2 = cl_conv8_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        cl_conv8_2 = cl_conv8_2.view(batch_size, -1, self.num_classes)\r\n",
    "\r\n",
    "        cl_conv9_2 = self.cl_conv9_2(conv9_2_feat)\r\n",
    "        cl_conv9_2 = cl_conv9_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        cl_conv9_2 = cl_conv9_2.view(batch_size, -1, self.num_classes)\r\n",
    "\r\n",
    "        cl_conv10_2 = self.cl_conv10_2(conv10_2_feat)\r\n",
    "        cl_conv10_2 = cl_conv10_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        cl_conv10_2 = cl_conv10_2.view(batch_size, -1, self.num_classes)\r\n",
    "\r\n",
    "        cl_conv11_2 = self.cl_conv11_2(conv11_2_feat)\r\n",
    "        cl_conv11_2 = cl_conv11_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        cl_conv11_2 = cl_conv11_2.view(batch_size, -1, self.num_classes)\r\n",
    "\r\n",
    "        \r\n",
    "        locs = torch.cat((loc_conv4_3, loc_conv7, loc_conv8_2, loc_conv9_2, loc_conv10_2, loc_conv11_2), dim=1) # dim: the dimention over which the tensors are concatnated\r\n",
    "        classifs = torch.cat((cl_conv4_3, cl_conv7, cl_conv8_2, cl_conv9_2, cl_conv10_2, cl_conv11_2), dim=1) \r\n",
    "\r\n",
    "        return locs, classifs\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def create_default_boxes():\r\n",
    "    fmap_dims = {'conv4_3': 38,\r\n",
    "                     'conv7': 19,\r\n",
    "                     'conv8_2': 10,\r\n",
    "                     'conv9_2': 5,\r\n",
    "                     'conv10_2': 3,\r\n",
    "                     'conv11_2': 1}\r\n",
    "\r\n",
    "    obj_scales = {'conv4_3': 0.1,\r\n",
    "                      'conv7': 0.2,\r\n",
    "                      'conv8_2': 0.375,\r\n",
    "                      'conv9_2': 0.55,\r\n",
    "                      'conv10_2': 0.725,\r\n",
    "                      'conv11_2': 0.9}\r\n",
    "\r\n",
    "    aspect_ratios = {'conv4_3': [1., 2., 0.5],\r\n",
    "                         'conv7': [1., 2., 3., 0.5, .333],\r\n",
    "                         'conv8_2': [1., 2., 3., 0.5, .333],\r\n",
    "                         'conv9_2': [1., 2., 3., 0.5, .333],\r\n",
    "                         'conv10_2': [1., 2., 0.5],\r\n",
    "                         'conv11_2': [1., 2., 0.5]}\r\n",
    "\r\n",
    "    fmaps = list(fmap_dims.keys())\r\n",
    "\r\n",
    "    prior_boxes = []\r\n",
    "\r\n",
    "    for k, fmap in enumerate(fmaps):\r\n",
    "        for i in range(fmap_dims[fmap]):\r\n",
    "            for j in range(fmap_dims[fmap]):\r\n",
    "                cx = (j + 0.5) / fmap_dims[fmap]\r\n",
    "                cy = (i + 0.5) / fmap_dims[fmap]\r\n",
    "\r\n",
    "                for ratio in aspect_ratios[fmap]:\r\n",
    "                    prior_boxes.append([cx, cy, obj_scales[fmap] * np.sqrt(ratio), obj_scales[fmap] / np.sqrt(ratio)])\r\n",
    "\r\n",
    "                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\r\n",
    "                        # scale of the current feature map and the scale of the next feature map\r\n",
    "                    if ratio == 1.:\r\n",
    "                        try:\r\n",
    "                            additional_scale = np.sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\r\n",
    "                            # For the last feature map, there is no \"next\" feature map\r\n",
    "                        except IndexError:\r\n",
    "                            additional_scale = 1.\r\n",
    "                        prior_boxes.append([cx, cy, additional_scale, additional_scale])\r\n",
    "\r\n",
    "    prior_boxes = torch.FloatTensor(prior_boxes)  # (8732, 4)\r\n",
    "    prior_boxes.clamp_(0, 1)  # (8732, 4)\r\n",
    "\r\n",
    "    return prior_boxes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "class SSD300(nn.Module):\r\n",
    "    def __init__(self, num_classes, aspect_ratios, features_size):\r\n",
    "        super(SSD300, self).__init__()\r\n",
    "\r\n",
    "        self.num_classes = num_classes\r\n",
    "\r\n",
    "        self.base = Base()\r\n",
    "        for param in self.base.parameters():\r\n",
    "            param.requires_grad = False\r\n",
    "            \r\n",
    "        self.extras = Extras()\r\n",
    "        self.predict = Predict(num_classes)\r\n",
    "\r\n",
    "        self.def_boxes = create_default_boxes()\r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, image):\r\n",
    "        conv4_3_feat, conv7_feat = self.base(image) #(N, 512, 38, 38), (N, 1024, 19, 19)\r\n",
    "\r\n",
    "        # L2 Norm \r\n",
    "        norm = conv4_3_feat.pow(2).sum(dim=1, keepdim=True) #(N, 1, 38, 38)\r\n",
    "        norm = torch.sqrt(norm)\r\n",
    "        conv4_3_feat = conv4_3_feat / norm #(N, 1, 38, 38)\r\n",
    "        conv4_3_feat = conv4_3_feat * 20 \r\n",
    "\r\n",
    "\r\n",
    "        conv8_2_feat, conv9_2_feat, conv10_2_feat, conv11_2_feat = self.extras(conv7_feat)\r\n",
    "        locs, confs = self.predict(conv4_3_feat, conv7_feat, conv8_2_feat, conv9_2_feat, conv10_2_feat, conv11_2_feat)\r\n",
    "\r\n",
    "        # detect = Detect()\r\n",
    "        # output = detect.forward(locs, confs, self.def_boxes)\r\n",
    "        output = (locs, confs, self.def_boxes)\r\n",
    "        return output\r\n",
    "\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# class DefaultBoxes():\r\n",
    "#     def __init__(self, aspect_ratios, features_size, scale):\r\n",
    "#         self.ratios = aspect_ratios #for each feature\r\n",
    "#         self.feat_size = feature_size\r\n",
    "#         self.scale = scale\r\n",
    "\r\n",
    "#     def create_dbox(self, m=6):\r\n",
    "#         \"\"\" Args:\r\n",
    "#                 m: number of feature maps for prediction,\r\n",
    "#                 s_max: max scale for each default bounding box,\r\n",
    "#                 s_min: min scale for each default bounding box,  \r\n",
    "\r\n",
    "#             Return: Tensor of default bounding boxes with (cx, cy, w, h)     \r\n",
    "#         \"\"\"\r\n",
    "\r\n",
    "#         def_boxes = []\r\n",
    "\r\n",
    "#         for k in range(m):\r\n",
    "#             s_k = self.scale[k]\r\n",
    "#             w = []\r\n",
    "#             h = []\r\n",
    "            \r\n",
    "#             for r in self.ratios[k]:\r\n",
    "#                 sqrt_r = np.sqrt(r)\r\n",
    "\r\n",
    "#                 temp_w = s_k * sqrt_r\r\n",
    "#                 temp_h = s_k / sqrt_r\r\n",
    "#                 w.append(temp_w)\r\n",
    "#                 h.append(temp_h)\r\n",
    "#                 if r == 1:\r\n",
    "#                     try:\r\n",
    "#                         s_ = np.sqrt(self.scale[k] * self.scale[k+1])\r\n",
    "#                     except IndexError:\r\n",
    "#                         S_ = 1.\r\n",
    "                    \r\n",
    "#                     temp_w = s_ * sqrt_r\r\n",
    "#                     temp_h = s_ / sqrt_r\r\n",
    "#                     w.append(temp_w)\r\n",
    "#                     h.append(temp_h)\r\n",
    "\r\n",
    "#             for i in range(0, self.feat_size[k]):\r\n",
    "#                 cy = (i + 0.5) / self.feat_size[k]\r\n",
    "\r\n",
    "#                 for j in range(0, self.feat_size[k]):\r\n",
    "#                     cx = (j + 0.5) / self.feat_size[k]\r\n",
    "                    \r\n",
    "#                     for l in range(len(w)):\r\n",
    "#                         temp = [cx, cy, w[l], h[l]]\r\n",
    "#                         def_boxes.append(temp)\r\n",
    "                        \r\n",
    "#         def_boxes = torch.tensor(def_boxes)\r\n",
    "#         def_boxes.clamp_(min=0, max=1)\r\n",
    "        \r\n",
    "#         return def_boxes\r\n",
    "\r\n",
    "# aspect_ratios = [[1, 2, 0.5], [1, 2, 3, 0.5, 0.333], [1, 2, 3, 0.5, 0.333], [1, 2, 3, 0.5, 0.333], [1, 2, 0.5], [1, 2, 0.5]]\r\n",
    "# feature_size = [38, 19, 10, 5, 3, 1]\r\n",
    "# scale = [0.1, 0.2, 0.375, 0.55, 0.725, 0.9]\r\n",
    "\r\n",
    "# defbox = DefaultBoxes(aspect_ratios, feature_size, scale)\r\n",
    "\r\n",
    "# defboxes = defbox.create_dbox()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def nms(boxes, scores, overlap=0.45, top_k=200):\r\n",
    "    \"\"\" boxes: (8732, 4)\r\n",
    "        scores: (8732, )\r\n",
    "        Giu 200 box co score cao nhat\r\n",
    "        Cac box co overlap > 0.45 so voi box co score cao nhat se bi loai\r\n",
    "\r\n",
    "        Return: tensor chua index của các box thỏa mãn\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    count = 0\r\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\r\n",
    "    # keep = []\r\n",
    "    \r\n",
    "    xmin = boxes[:, 0] # (8732, )\r\n",
    "    ymin = boxes[:, 1]\r\n",
    "    xmax = boxes[:, 2]\r\n",
    "    ymax = boxes[:, 3]\r\n",
    "\r\n",
    "    area = (xmax - xmin) * (ymax - ymin)\r\n",
    "\r\n",
    "    _, idx = torch.sort(scores, dim=0, )\r\n",
    "\r\n",
    "    idx = idx[-top_k:]\r\n",
    "    \r\n",
    "\r\n",
    "    while(idx.numel() > 0):\r\n",
    "\r\n",
    "        i = idx[-1] #Lấy index của box có score cao nhất\r\n",
    "        keep[count] = i\r\n",
    "        count += 1\r\n",
    "        # keep.append(i)\r\n",
    "\r\n",
    "        if idx.size(0) == 1:\r\n",
    "            break\r\n",
    "\r\n",
    "        idx = idx[:-1]\r\n",
    "\r\n",
    "        temp_xmin = torch.index_select(input=xmin, dim=0, index=idx) # (199,)\r\n",
    "        temp_ymin = torch.index_select(input=ymin, dim=0, index=idx)\r\n",
    "        temp_xmax = torch.index_select(input=xmax, dim=0, index=idx)\r\n",
    "        temp_ymax = torch.index_select(input=ymax, dim=0, index=idx)\r\n",
    "\r\n",
    "        temp_xmin.clamp_(min=xmin[i]) #(199, )\r\n",
    "        temp_ymin.clamp_(min=ymin[i])\r\n",
    "        temp_xmax.clamp_(max=xmax[i])\r\n",
    "        temp_ymax.clamp_(max=ymax[i])\r\n",
    "        \r\n",
    "        temp_w = temp_xmax - temp_xmin #(199, )\r\n",
    "        temp_h = temp_ymax - temp_ymin\r\n",
    "\r\n",
    "        temp_w.clamp_(min=0.) # (199, )\r\n",
    "        temp_h.clamp_(min=0.)\r\n",
    "\r\n",
    "        intersect = temp_w * temp_h  #(199, )\r\n",
    "        others_area = torch.index_select(input=area, dim=0, index=idx) # (199, )\r\n",
    "\r\n",
    "        union = area[i] + others_area - 2 * intersect\r\n",
    "\r\n",
    "        iou = intersect / union \r\n",
    "\r\n",
    "        idx = idx[torch.le(iou, overlap)]\r\n",
    "\r\n",
    "    # keep = torch.tensor(keep)\r\n",
    "        \r\n",
    "    return keep, count\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "boxes = np.random.randint(1, 300, (8732,4))\r\n",
    "boxes = torch.from_numpy(boxes)\r\n",
    "\r\n",
    "scores = torch.rand(8732)\r\n",
    "\r\n",
    "keep, count = nms(boxes, scores)\r\n",
    "\r\n",
    "print(count)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "189\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def decode(locs, def_boxes):\r\n",
    "    \"\"\" Tinhs bouding boxes\r\n",
    "        locs: (8732, 4)          offsets ---> center form ---> xymin xymax form\r\n",
    "        def_boxes: (8732, 4)\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    boxes = torch.cat((\r\n",
    "        def_boxes[:, :2] + locs[:, :2] * def_boxes[:, 2:] * 0.1, #cx, cy\r\n",
    "        def_boxes[:, 2:] * torch.exp(locs[:, 2:] * 0.2)), dim=1) #w, h\r\n",
    "\r\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2  #xmin, ymin\r\n",
    "    boxes[:, 2:] += boxes[:, :2]  # xmax, ymax\r\n",
    " \r\n",
    "    return boxes\r\n",
    "\r\n",
    "def encode(matches, def_boxes):\r\n",
    "    \"\"\" Biến truth box của từng def_box về dạng offset cho mô hình\r\n",
    "        Args:\r\n",
    "            matches: (8732, 4)      xymin xymax form ---> center form ---> offset\r\n",
    "            def_boxes: (8732, 4)\r\n",
    "        Return:\r\n",
    "            offset: (8732, 4) \r\n",
    "    \"\"\"\r\n",
    "    g_cxcy = (matches[:, :2] + matches[:, 2:]) / 2 #cxcy\r\n",
    "    g_wh = matches[:, 2:] - matches[:, :2]\r\n",
    "    \r\n",
    "    g_hat_cxcy = (g_cxcy - def_boxes[:, :2]) / (def_boxes[:, 2:] * 0.1)\r\n",
    "    g_hat_wh = torch.log(g_wh / def_boxes[:, 2:]) / 0.2 \r\n",
    "\r\n",
    "    locs = torch.cat([g_hat_cxcy, g_hat_wh], dim=1)\r\n",
    "    return locs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "a = torch.rand((2, 4))\r\n",
    "b = torch.rand((2, 4))\r\n",
    "print(a)\r\n",
    "print(b)\r\n",
    "\r\n",
    "c = decode(a, b)\r\n",
    "print(c)\r\n",
    "\r\n",
    "d = encode(c, b)\r\n",
    "print(d)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.8490, 0.3254, 0.9618, 0.2967],\n",
      "        [0.7611, 0.3019, 0.9581, 0.1498]])\n",
      "tensor([[0.4356, 0.3859, 0.4768, 0.6770],\n",
      "        [0.6621, 0.1978, 0.5110, 0.2030]])\n",
      "tensor([[0.1871, 0.0488, 0.7650, 0.7672],\n",
      "        [0.3915, 0.0993, 1.0104, 0.3085]])\n",
      "tensor([[0.8490, 0.3254, 0.9618, 0.2967],\n",
      "        [0.7611, 0.3019, 0.9581, 0.1498]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "class Detect():\r\n",
    "    def __init__(self, conf_thresh=0.01, top_k=200, nms_thresh=0.45):\r\n",
    "        self.softmax = nn.Softmax(dim=-1)\r\n",
    "        self.conf_thresh = conf_thresh\r\n",
    "        self.top_k = top_k\r\n",
    "        self.nms_thresh = nms_thresh\r\n",
    "\r\n",
    "    def forward(self, locs, confs, def_boxes):\r\n",
    "        batch_size = confs.size(0)\r\n",
    "        num_bbox = confs.size(1)\r\n",
    "        num_class = confs.size(2)\r\n",
    "\r\n",
    "        confs = self.softmax(confs) #(batch_size, 8732, num_class)\r\n",
    "        # print('confs = ', confs.shape)\r\n",
    "        confs_pred = confs.permute(0, 2, 1).contiguous() #(batch_size, num_class, 8732)\r\n",
    "        # print(confs_pred.shape)\r\n",
    "\r\n",
    "        output = torch.zeros(batch_size, num_class, self.top_k, 5)\r\n",
    "\r\n",
    "        # Xuwr lys tuwngf anhr\r\n",
    "        for i in range(batch_size):\r\n",
    "            decode_boxes = decode(locs[i], def_boxes) # (8732, 4)\r\n",
    "\r\n",
    "            confs_score = confs_pred[i].clone().detach() # (num_class, 8732)\r\n",
    "\r\n",
    "            for cl in range(1, num_class):  # Bỏ background\r\n",
    "                c_mask = confs_score[cl].gt(self.conf_thresh) # laays nhuwngx thawngf lonws hown 0.01 \r\n",
    "                scores = confs_score[cl][c_mask] #list\r\n",
    "                # print(c_mask.shape)\r\n",
    "                # print(scores.shape)\r\n",
    "                if scores.numel() == 0:\r\n",
    "                    continue\r\n",
    "                \r\n",
    "                l_mask = c_mask.unsqueeze(1).expand_as(decode_boxes) # (8732, 4)\r\n",
    "                # print(l_mask.shape)\r\n",
    "                boxes = decode_boxes[l_mask].view(-1, 4) # (abc, 4)\r\n",
    "                # print(boxes.shape)\r\n",
    "                ids, count = nms(boxes, scores, overlap=self.nms_thresh, top_k=self.top_k)\r\n",
    "\r\n",
    "                # count = len(ids)\r\n",
    "                output[i, cl, :count] = torch.cat((scores[ids[:count]].unsqueeze(1), boxes[ids[:count]]), 1)\r\n",
    "            #     print('---'*20)\r\n",
    "            # print('---'*20)\r\n",
    "        return output\r\n",
    "                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def intersect(box_a, box_b):\r\n",
    "    \"\"\"Args:\r\n",
    "        box_a : tensor (num_boxes_a, 4)\r\n",
    "        box_b : tensor (num_boxes_b, 4)\r\n",
    "\r\n",
    "       Return:\r\n",
    "        intersection area: tensor (num_boxes_A, num_boxes_B)\r\n",
    "    \"\"\"\r\n",
    "    A = box_a.size(0)\r\n",
    "    B = box_b.size(0)\r\n",
    "\r\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2), box_b[:, :2].unsqueeze(0).expand(A, B, 2))\r\n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2), box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\r\n",
    "\r\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\r\n",
    "    \r\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def jaccard(box_a, box_b):\r\n",
    "    \"\"\"\r\n",
    "    \"\"\" \r\n",
    "    inter = intersect(box_a, box_b) # (num_boxes_a, num_boxes_b)\r\n",
    "    area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]) #(num_boxes_a, )\r\n",
    "    area_b = (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]) #(num_boxes_b, )\r\n",
    "\r\n",
    "    area_a.unsqueeze_(1).expand_as(inter)\r\n",
    "    area_b.unsqueeze_(0).expand_as(inter)\r\n",
    "\r\n",
    "    union = area_a + area_b - 2 * inter\r\n",
    "\r\n",
    "    return inter / union"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def cxcy_to_xy(boxes):\r\n",
    "    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\r\n",
    "    representation for comparison to point form ground truth data.\r\n",
    "    Args:\r\n",
    "        boxes: (tensor) center-size default boxes from priorbox layers.\r\n",
    "    Return:\r\n",
    "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\r\n",
    "    \"\"\"\r\n",
    "    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\r\n",
    "                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def match(threshhold, truths, def_boxes, labels, locs_t, confs_t, idx):\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "        Match each prior box with the ground truth box of the highest jaccard\r\n",
    "    overlap, encode the bounding boxes, then return the matched indices\r\n",
    "    corresponding to both confidence and location preds.\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    dbox_xy = cxcy_to_xy(def_boxes) # to xmin ymin xmax ymax (8732, 4)\r\n",
    "\r\n",
    "    overlap = jaccard(truths, dbox_xy) # (num_truth_boxes, 8732)\r\n",
    "    print(overlap.shape)\r\n",
    "\r\n",
    "    best_dbox_overlap, best_dbox_idx = torch.max(overlap, dim=1) # (num_truth_boxes,)\r\n",
    "    best_truth_overlap, best_truth_idx = torch.max(overlap, dim=0) #(8732,)\r\n",
    "\r\n",
    "    # best_truth_overlap.index_fill_(0, best_dbox_idx, 2)  # to ensure best dbox\r\n",
    "\r\n",
    "    # for j in range(best_dbox_idx.size(0)):\r\n",
    "    #     best_truth_idx[best_dbox_idx[j]] = j\r\n",
    "\r\n",
    "    matches = truths[best_truth_idx]  # (8732, 4)\r\n",
    "    confs = labels[best_truth_idx] + 1 # set label from truth box to each dbox (8732,)\r\n",
    "    confs[best_truth_overlap < threshhold] = 0 # set background to 0 #(8732, )\r\n",
    "\r\n",
    "    locs = encode(matches, def_boxes)  # (8732, 4)\r\n",
    "    locs_t[idx] = locs \r\n",
    "    confs_t[idx] = confs\r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "class MultiboxLoss(nn.Module):\r\n",
    "    \"\"\" \r\n",
    "        1. localization loss\r\n",
    "        2. confidence loss for predict class score\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, threshold=0.5, neg_pos_ratio=3, alpha=1):\r\n",
    "        super(MultiboxLoss, self).__init__()\r\n",
    "\r\n",
    "        self.threshold = threshold\r\n",
    "        self.neg_pos_ratio = neg_pos_ratio\r\n",
    "        self.alpha = alpha\r\n",
    "        self.L1Loss = nn.SmoothL1Loss(reduction='sum')\r\n",
    "        self.CrossentropyLoss = nn.CrossEntropyLoss(reduction='none')\r\n",
    "\r\n",
    "    def forward(self, predictions, targets):\r\n",
    "        locs, confs, def_boxes = predictions # (batch, 8732, 4), (batch, 8732, num_classes), (8732, 4)\r\n",
    "\r\n",
    "        batch_size = confs.size(0)\r\n",
    "        num_boxes = confs.size(1)\r\n",
    "        num_classes = confs.size(2)\r\n",
    "\r\n",
    "        confs_t_labels = torch.LongTensor(batch_size, num_boxes)\r\n",
    "        locs_t = torch.Tensor(batch_size, num_boxes, 4)\r\n",
    "\r\n",
    "        for i in range(batch_size):\r\n",
    "            truths = targets[i][:, :-1] # xmin, ymin, xmax, ymax\r\n",
    "            labels = targets[i][:, -1] # label\r\n",
    "            match(self.threshold, truths, def_boxes, labels, locs_t, confs_t_labels, i)\r\n",
    "\r\n",
    "    #Loc_loss\r\n",
    "        pos_mask = confs_t_labels > 0  #(batch, 8732)\r\n",
    "        pos_idx = pos_mask.unsqueeze(2).expand_as(locs_t) #(batch, 8732, 4)\r\n",
    "\r\n",
    "        locs_p = locs[pos_idx]\r\n",
    "        locs_t = locs_t[pos_idx]\r\n",
    "\r\n",
    "        # print(pos_mask.shape)\r\n",
    "        # print(locs.shape)\r\n",
    "        # print(locs_p.shape)\r\n",
    "        # print(locs_t.shape)\r\n",
    "        \r\n",
    "        loc_loss = self.L1Loss(locs_p, locs_t)\r\n",
    "        # print(loc_loss)\r\n",
    "\r\n",
    "    # Conf_loss\r\n",
    "        true_classes = confs_t_labels.view(-1) #(8732)\r\n",
    "        predicted_scores = confs.view(-1, num_classes)\r\n",
    "\r\n",
    "        conf_loss_all = self.CrossentropyLoss(predicted_scores, true_classes) # (batch * 8732)\r\n",
    "        conf_loss_all = conf_loss_all.view(batch_size, -1) # (batch, 8732)\r\n",
    "\r\n",
    "        # hard negative mining\r\n",
    "        n_positive = pos_mask.sum(dim=1) # (batch,)\r\n",
    "        # print(n_positive.shape)\r\n",
    "        n_hard_negative = torch.clamp(n_positive*self.neg_pos_ratio, max=num_boxes) # (batch, )\r\n",
    "\r\n",
    "        conf_loss_pos = conf_loss_all[pos_mask] #(N, )\r\n",
    "\r\n",
    "        conf_loss_neg = conf_loss_all.clone()\r\n",
    "        conf_loss_neg[pos_mask] = 0.  # chuyển loss của positive về 0\r\n",
    "\r\n",
    "        conf_loss_neg, _ = torch.sort(conf_loss_neg, dim=1, descending=True)\r\n",
    "\r\n",
    "        loss_rank = torch.LongTensor(range(num_boxes)).unsqueeze(0).expand_as(conf_loss_neg) # (batch, 8732)\r\n",
    "        hard_negative = loss_rank < n_hard_negative.unsqueeze(1).expand_as(loss_rank)\r\n",
    "\r\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negative] \r\n",
    "\r\n",
    "        conf_loss = (conf_loss_pos.sum() + conf_loss_hard_neg.sum()) \r\n",
    "        # print(conf_loss)\r\n",
    "\r\n",
    "        loss = (conf_loss + self.alpha * loc_loss) / n_positive.sum().float()\r\n",
    "        loss.nan_to_num_(nan=0) # set NaN value to 0\r\n",
    "\r\n",
    "        return loss\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "x = cv2.imread('000007.jpg')\r\n",
    "x = cv2.resize(x, (300, 300))\r\n",
    "x = torch.from_numpy(x)\r\n",
    "x.unsqueeze_(0)\r\n",
    "x = x.reshape(1, 3, 300, 300) / 255.\r\n",
    "\r\n",
    "aspect_ratios = [[1, 2, 0.5], [1, 2, 3, 0.5, 0.333], [1, 2, 3, 0.5, 0.333], [1, 2, 3, 0.5, 0.333], [1, 2, 0.5], [1, 2, 0.5]]\r\n",
    "feature_size = [38, 19, 10, 5, 3, 1]\r\n",
    "\r\n",
    "\r\n",
    "def_boxes = create_default_boxes()\r\n",
    "\r\n",
    "model = SSD300(21, aspect_ratios, feature_size)\r\n",
    "# print(model)\r\n",
    "print('Trainable params =',sum(p.numel() for p in model.parameters() if p.requires_grad))\r\n",
    "print('Total params =',sum(p.numel() for p in model.parameters()))\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained model VGG to Base.\n",
      "Trainable params = 5570670\n",
      "Total params = 26054574\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "criterion = MultiboxLoss()\r\n",
    "\r\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "a = torch.rand(5, 3, 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "\r\n",
    "model.train()\r\n",
    "\r\n",
    "outputs = model(x)\r\n",
    "loss = criterion(predictions=outputs, targets=a)\r\n",
    "optim.zero_grad()\r\n",
    "loss.backward()\r\n",
    "optim.step()\r\n",
    "print(loss.item())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 8732])\n",
      "29.19084358215332\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "locs = outputs[0]\r\n",
    "\r\n",
    "boxes = decode(locs.reshape(8732, 4), def_boxes)\r\n",
    "\r\n",
    "boxes[:20]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.4460e-02, -7.0942e-03,  9.6465e-02,  9.1003e-02],\n",
       "        [-9.1487e-03, -1.8589e-02,  1.1224e-01,  7.7940e-02],\n",
       "        [-4.4530e-02,  6.7659e-04,  6.5434e-02,  6.5955e-02],\n",
       "        [-1.3981e-02, -4.1978e-02,  8.0278e-02,  1.1020e-01],\n",
       "        [-5.9685e-03, -2.1212e-03,  1.5346e-01,  8.9949e-02],\n",
       "        [ 2.5840e-02, -2.8026e-02,  1.3652e-01,  8.7384e-02],\n",
       "        [-2.3912e-02,  1.4918e-02,  9.1397e-02,  6.9721e-02],\n",
       "        [ 4.2217e-02, -8.1630e-02,  1.0384e-01,  9.5648e-02],\n",
       "        [ 2.4487e-02, -9.2654e-05,  1.4944e-01,  8.9629e-02],\n",
       "        [ 5.6176e-02, -2.9259e-02,  1.7205e-01,  8.5613e-02],\n",
       "        [ 8.5417e-03,  8.1539e-03,  1.2121e-01,  6.9934e-02],\n",
       "        [ 5.4487e-02, -5.6864e-02,  1.2478e-01,  8.9103e-02],\n",
       "        [ 5.3006e-02,  2.8352e-02,  1.6032e-01,  8.9078e-02],\n",
       "        [ 6.5226e-02, -2.4195e-02,  1.8635e-01,  8.5914e-02],\n",
       "        [ 3.0349e-02,  1.3200e-02,  1.2311e-01,  6.4026e-02],\n",
       "        [ 7.1048e-02, -6.8873e-02,  1.4475e-01,  8.5478e-02],\n",
       "        [ 9.7067e-02,  3.8247e-02,  1.6592e-01,  9.3648e-02],\n",
       "        [ 1.3114e-01, -3.9088e-02,  2.2465e-01,  9.0269e-02],\n",
       "        [ 4.5027e-02, -6.0514e-03,  1.4810e-01,  6.3470e-02],\n",
       "        [ 1.1878e-01, -4.7895e-02,  1.7583e-01,  1.1054e-01]],\n",
       "       dtype=torch.float64, grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\r\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26054574\n",
      "26054574\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "w = torch.empty(3, 5)\r\n",
    "nn.init.uniform_(w, a=1, b=2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.9320, 1.5141, 1.3847, 1.5824, 1.8143],\n",
       "        [1.9848, 1.9508, 1.9867, 1.4063, 1.8686],\n",
       "        [1.8464, 1.7952, 1.2754, 1.9279, 1.8279]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "aspect_ratios = [[1, 2, 0.5], [1, 2, 3, 0.5, 0.333], [1, 2, 3, 0.5, 0.333], [1, 2, 3, 0.5, 0.333], [1, 2, 0.5], [1, 2, 0.5]]\r\n",
    "feature_size = [38, 19, 10, 5, 3, 1]\r\n",
    "\r\n",
    "\r\n",
    "defbox = DefaultBoxes(aspect_ratios, feature_size)\r\n",
    "\r\n",
    "defbox_tensor = defbox.create_dbox()\r\n",
    "\r\n",
    "df = pd.DataFrame(defbox_tensor.numpy())\r\n",
    "\r\n",
    "df.head(15)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           0         1         2         3\n",
       "0   0.039474  0.039474  0.200000  0.200000\n",
       "1   0.039474  0.039474  1.000000  1.000000\n",
       "2   0.039474  0.039474  0.282843  0.141421\n",
       "3   0.039474  0.039474  0.141421  0.282843\n",
       "4   0.065789  0.039474  0.200000  0.200000\n",
       "5   0.065789  0.039474  1.000000  1.000000\n",
       "6   0.065789  0.039474  0.282843  0.141421\n",
       "7   0.065789  0.039474  0.141421  0.282843\n",
       "8   0.092105  0.039474  0.200000  0.200000\n",
       "9   0.092105  0.039474  1.000000  1.000000\n",
       "10  0.092105  0.039474  0.282843  0.141421\n",
       "11  0.092105  0.039474  0.141421  0.282843\n",
       "12  0.118421  0.039474  0.200000  0.200000\n",
       "13  0.118421  0.039474  1.000000  1.000000\n",
       "14  0.118421  0.039474  0.282843  0.141421"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.141421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.282843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.141421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.282843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.141421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.282843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.141421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "ratios = [[1, 2, 0.5], [1, 2, 3, 0.5, 0.333], [1, 2, 3, 0.5, 0.333], [1, 2, 3, 0.5, 0.333], [1, 2, 0.5], [1, 2, 0.5]]\r\n",
    "feat_size = [38, 19, 10, 5, 3, 1]\r\n",
    "\r\n",
    "def_box = []\r\n",
    "m = 6\r\n",
    "s_max = 0.9\r\n",
    "s_min = 0.2\r\n",
    "for k in range(m):\r\n",
    "    s_k = s_min + (s_max - s_min) * k / (m - 1)\r\n",
    "    w = []\r\n",
    "    h = []\r\n",
    "    for r in ratios[k]:\r\n",
    "        sqrt_r = np.sqrt(r)\r\n",
    "\r\n",
    "        temp_w = s_k * sqrt_r\r\n",
    "        temp_h = s_k / sqrt_r\r\n",
    "        w.append(temp_w)\r\n",
    "        h.append(temp_h)\r\n",
    "        if r == 1:\r\n",
    "            temp_w = np.sqrt(s_k**2+1) * sqrt_r\r\n",
    "            temp_h = np.sqrt(s_k**2+1) * sqrt_r\r\n",
    "            w.append(temp_w)\r\n",
    "            h.append(temp_h)\r\n",
    "    print('w = ', w)\r\n",
    "    print('h = ', h)\r\n",
    "\r\n",
    "    for i in range(1, feat_size[k]+1):\r\n",
    "        cy = (i + 0.5) / feat_size[k]\r\n",
    "        for j in range(1, feat_size[k]+1):\r\n",
    "            cx = (j + 0.5) / feat_size[k]\r\n",
    "            for l in range(len(w)):\r\n",
    "                temp = [cx, cy, w[l], h[l]]\r\n",
    "                def_box.append(temp)\r\n",
    "                # print(temp)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w =  [0.2, 1.019803902718557, 0.28284271247461906, 0.14142135623730953]\n",
      "h =  [0.2, 1.019803902718557, 0.1414213562373095, 0.282842712474619]\n",
      "w =  [0.33999999999999997, 1.0562196741208714, 0.4808326112068523, 0.5888972745734182, 0.24041630560342614, 0.19620091742904772]\n",
      "h =  [0.33999999999999997, 1.0562196741208714, 0.2404163056034261, 0.19629909152447275, 0.4808326112068522, 0.5891919442313744]\n",
      "w =  [0.48, 1.1092339699089637, 0.6788225099390857, 0.831384387633061, 0.33941125496954283, 0.27698953048806735]\n",
      "h =  [0.48, 1.1092339699089637, 0.3394112549695428, 0.27712812921102037, 0.6788225099390855, 0.831800391856058]\n",
      "w =  [0.6199999999999999, 1.176605286406618, 0.8768124086713188, 1.0738715006927038, 0.4384062043356594, 0.357778143547087]\n",
      "h =  [0.6199999999999999, 1.176605286406618, 0.43840620433565936, 0.3579571668975679, 0.8768124086713187, 1.0744088394807416]\n",
      "w =  [0.76, 1.2560254774486066, 1.0748023074035524, 0.5374011537017762]\n",
      "h =  [0.76, 1.2560254774486066, 0.5374011537017761, 1.0748023074035522]\n",
      "w =  [0.8999999999999999, 1.345362404707371, 1.2727922061357855, 0.6363961030678927]\n",
      "h =  [0.8999999999999999, 1.345362404707371, 0.6363961030678926, 1.2727922061357853]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def_box = torch.tensor(def_box)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def_box.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([8732, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "bda2dbd333357889da984cf9be9c50cb5e8115445fa270a04925ab0de91782a8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}