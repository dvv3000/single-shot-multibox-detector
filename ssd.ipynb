{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np \r\n",
    "import pandas as pd \r\n",
    "import matplotlib.pyplot as plt \r\n",
    "import random\r\n",
    "import cv2\r\n",
    "import os\r\n",
    "\r\n",
    "import torch\r\n",
    "import torchvision\r\n",
    "import torch.nn as nn\r\n",
    "import torchvision.transforms as transforms\r\n",
    "\r\n",
    "from torch.autograd import Function # khi gọi class, nó tự động tính hàm foward()\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "\r\n",
    "import xml.etree.ElementTree as ET"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(\"device:\", device)\r\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Base(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Base, self).__init__()\r\n",
    "\r\n",
    "        self.layer1 = nn.Sequential(\r\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), # (N, 64, 300, 300)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), # (N, 64, 300, 300)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # (N, 64, 150, 150)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer2 = nn.Sequential(\r\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), #(N, 128, 150, 150)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), # (N, 128, 150, 150)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # (N, 128, 75, 75)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer3 = nn.Sequential(\r\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), # (N, 256, 75, 75)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), # (N, 256, 75, 75)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), # (N, 256, 75, 75)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True) # (N, 256, 38, 38)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer4 = nn.Sequential(\r\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1), # (N, 512, 38, 38)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (N, 512, 38, 38)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (N, 512, 38, 38)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # (N, 512, 19, 19)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer5 = nn.Sequential(\r\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (N, 512, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (N, 512, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (N, 512, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1) # (N, 512, 19, 19)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer6 = nn.Sequential(\r\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6), # (N, 1024, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "        \r\n",
    "        self.layer7 = nn.Sequential(\r\n",
    "            nn.Conv2d(1024, 1024, kernel_size=1), # (N, 1024, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "\r\n",
    "        self.load_pretrained_params()\r\n",
    "\r\n",
    "    def forward(self, input):\r\n",
    "        #Base\r\n",
    "        out = self.layer1(input)\r\n",
    "        out = self.layer2(out)\r\n",
    "        out = self.layer3(out)\r\n",
    "        conv4_3_feat = out\r\n",
    "        \r\n",
    "        out = self.layer4(out)\r\n",
    "        out = self.layer5(out)\r\n",
    "        out = self.layer6(out)\r\n",
    "        out = self.layer7(out)\r\n",
    "        conv7_feat = out\r\n",
    "\r\n",
    "        return conv4_3_feat, conv7_feat \r\n",
    "\r\n",
    "    def load_pretrained_params(self):\r\n",
    "\r\n",
    "        state_dict = self.state_dict()\r\n",
    "        params_keys = list(state_dict.keys())\r\n",
    "\r\n",
    "        vgg = torchvision.models.vgg16(pretrained=True)\r\n",
    "            \r\n",
    "        pretrained_state_dict = vgg.state_dict()\r\n",
    "        pretrained_params_keys = list(pretrained_state_dict.keys())\r\n",
    "\r\n",
    "        for i, key in enumerate(params_keys[:-4]):\r\n",
    "            state_dict[key] = pretrained_state_dict[pretrained_params_keys[i]]\r\n",
    "\r\n",
    "\r\n",
    "        #Convert fc6, fc7 to convolutional layers\r\n",
    "        w_fc6 = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)\r\n",
    "        b_fc6 = pretrained_state_dict['classifier.0.bias'] # (4096,)\r\n",
    "\r\n",
    "        w_fc7 = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)\r\n",
    "        b_fc7 = pretrained_state_dict['classifier.3.bias'] #(4096, )\r\n",
    "\r\n",
    "        # Subsample parameters of fc6, fc7\r\n",
    "        w_conv6 = torch.index_select(input=w_fc6, dim=0, index=torch.arange(0, 4096, step=4)) # (1024, 512, 7, 7)\r\n",
    "        w_conv6 = torch.index_select(input=w_conv6, dim=2, index=torch.arange(0, 7, step=3)) # (1024, 512, 3, 7)\r\n",
    "        w_conv6 = torch.index_select(input=w_conv6, dim=3, index=torch.arange(0, 7, step=3)) #(1024, 512, 3, 3)\r\n",
    "        \r\n",
    "        b_conv6 = torch.index_select(input=b_fc6, dim=0, index=torch.arange(0, 4096, step=4)) #(1024,)\r\n",
    "\r\n",
    "\r\n",
    "        w_conv7 = torch.index_select(input=w_fc7, dim=0, index=torch.arange(0, 4096, step=4)) #(1024, 4096, 1, 1)\r\n",
    "        w_conv7 = torch.index_select(input=w_conv7, dim=1, index=torch.arange(0, 4096, step=4)) #(1024, 1024, 1, 1)\r\n",
    "\r\n",
    "        b_conv7 = b_conv6 = torch.index_select(input=b_fc6, dim=0, index=torch.arange(0, 4096, step=4)) #(1024,)\r\n",
    "\r\n",
    "\r\n",
    "        state_dict['layer6.0.weight'] = w_conv6\r\n",
    "        state_dict['layer6.0.bias'] = b_conv6\r\n",
    "        state_dict['layer7.0.weight'] = w_conv7\r\n",
    "        state_dict['layer7.0.bias'] = b_conv7\r\n",
    "\r\n",
    "        self.load_state_dict(state_dict)\r\n",
    "\r\n",
    "        print('Loaded pretrained model VGG to Base.') \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Extras(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Extras, self).__init__()\r\n",
    "        self.layer8 = nn.Sequential(\r\n",
    "            nn.Conv2d(1024, 256, kernel_size=1, padding=0), #(N, 256, 19, 19)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2), # (N, 512, 10, 10)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer9 = nn.Sequential(\r\n",
    "            nn.Conv2d(512, 128, kernel_size=1, padding=0), # (N, 128, 10, 10)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1), # (N, 256, 5, 5)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "\r\n",
    "        self.layer10 = nn.Sequential(\r\n",
    "            nn.Conv2d(256, 128, kernel_size=1, padding=0), #(N, 128, 5, 5)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=0), # (N, 256, 3, 3)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "        \r\n",
    "        self.layer11 = nn.Sequential(\r\n",
    "            nn.Conv2d(256, 128, kernel_size=1, padding=0), #(N, 128, 3, 3)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=0), #(N, 256, 1, 1)\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "        )\r\n",
    "\r\n",
    "        self.init_params()\r\n",
    "\r\n",
    "    def init_params(self):\r\n",
    "        for c in self.children():\r\n",
    "            if isinstance(c, nn.Conv2d):\r\n",
    "                nn.init.xavier_uniform_(c.weight)\r\n",
    "                nn.init.constant_(c.bias, 0.)\r\n",
    "        \r\n",
    "    def forward(self, input):\r\n",
    "        #Extras\r\n",
    "        out = self.layer8(input)\r\n",
    "        conv8_2_feat = out\r\n",
    "        out = self.layer9(out)\r\n",
    "        conv9_2_feat = out\r\n",
    "        out = self.layer10(out)\r\n",
    "        conv10_2_feat = out\r\n",
    "        conv11_2_feat = self.layer11(out)\r\n",
    "\r\n",
    "        return conv8_2_feat, conv9_2_feat, conv10_2_feat, conv11_2_feat\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Predict(nn.Module):\r\n",
    "    def __init__(self, num_classes):\r\n",
    "        super(Predict, self).__init__()\r\n",
    "        self.num_classes = num_classes\r\n",
    "        num_boxes = {'conv4_3':4, 'conv7':6, 'conv8_2':6, 'conv9_2':6, 'conv10_2':4, 'conv11_2':4} #Number of default boxes for each feature\r\n",
    "\r\n",
    "        #Location\r\n",
    "        self.loc_conv4_3 = nn.Conv2d(256, num_boxes['conv4_3']*4, kernel_size=3, padding=1)\r\n",
    "        self.loc_conv7 = nn.Conv2d(1024, num_boxes['conv7']*4, kernel_size=3, padding=1)\r\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, num_boxes['conv8_2']*4, kernel_size=3, padding=1)\r\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, num_boxes['conv9_2']*4, kernel_size=3, padding=1)\r\n",
    "        self.loc_conv10_2 = nn.Conv2d(256, num_boxes['conv10_2']*4, kernel_size=3, padding=1)\r\n",
    "        self.loc_conv11_2 = nn.Conv2d(256, num_boxes['conv11_2']*4, kernel_size=3, padding=1)\r\n",
    "\r\n",
    "        #Classify\r\n",
    "        self.cl_conv4_3 = nn.Conv2d(256, num_boxes['conv4_3']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "        self.cl_conv7 = nn.Conv2d(1024, num_boxes['conv7']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, num_boxes['conv8_2']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, num_boxes['conv9_2']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "        self.cl_conv10_2 = nn.Conv2d(256, num_boxes['conv10_2']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "        self.cl_conv11_2 = nn.Conv2d(256, num_boxes['conv11_2']*self.num_classes, kernel_size=3, padding=1)\r\n",
    "\r\n",
    "        self.init_params()\r\n",
    "\r\n",
    "    def init_params(self):\r\n",
    "        for c in self.children():\r\n",
    "            if isinstance(c, nn.Conv2d):\r\n",
    "                nn.init.xavier_uniform_(c.weight)\r\n",
    "                nn.init.constant_(c.bias, 0.)\r\n",
    "    \r\n",
    "    \r\n",
    "    '''If you just want to reshape tensors, use torch.reshape.\r\n",
    "       If you're also concerned about memory usage and want to ensure that the two tensors share the same data, use torch.view.'''\r\n",
    "       \r\n",
    "    def forward(self, conv4_3_feat, conv7_feat, conv8_2_feat, conv9_2_feat, conv10_2_feat, conv11_2_feat):\r\n",
    "        batch_size = conv4_3_feat.shape[0]\r\n",
    "\r\n",
    "        #Location\r\n",
    "        loc_conv4_3 = self.loc_conv4_3(conv4_3_feat) # (N, 16, 38, 38)\r\n",
    "        loc_conv4_3 = loc_conv4_3.permute(0, 2, 3, 1).contiguous() # (N, 38, 38, 16)\r\n",
    "        loc_conv4_3 = loc_conv4_3.view(batch_size, -1, 4) #(N, 5776, 4)\r\n",
    "        \r\n",
    "        loc_conv7 = self.loc_conv7(conv7_feat) #(N, 24, 19, 19)\r\n",
    "        loc_conv7 = loc_conv7.permute(0, 2, 3, 1).contiguous() #(N, 19, 19, 24)\r\n",
    "        loc_conv7 = loc_conv7.view(batch_size, -1, 4) #(N, 2166, 4)\r\n",
    "\r\n",
    "        loc_conv8_2 = self.loc_conv8_2(conv8_2_feat)\r\n",
    "        loc_conv8_2 = loc_conv8_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        loc_conv8_2 = loc_conv8_2.view(batch_size, -1, 4)\r\n",
    "        \r\n",
    "        loc_conv9_2 = self.loc_conv9_2(conv9_2_feat)\r\n",
    "        loc_conv9_2 = loc_conv9_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        loc_conv9_2 = loc_conv9_2.view(batch_size, -1, 4)\r\n",
    "\r\n",
    "        loc_conv10_2 = self.loc_conv10_2(conv10_2_feat)\r\n",
    "        loc_conv10_2 = loc_conv10_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        loc_conv10_2 = loc_conv10_2.view(batch_size, -1, 4)\r\n",
    "\r\n",
    "        loc_conv11_2 = self.loc_conv11_2(conv11_2_feat)\r\n",
    "        loc_conv11_2 = loc_conv11_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        loc_conv11_2 = loc_conv11_2.view(batch_size, -1, 4)\r\n",
    "\r\n",
    "        #Classification\r\n",
    "        cl_conv4_3 = self.cl_conv4_3(conv4_3_feat)  #(N, classes*4, 38, 38)\r\n",
    "        cl_conv4_3 = cl_conv4_3.permute(0, 2, 3, 1).contiguous() #(N, 38, 38, classes*4)\r\n",
    "        cl_conv4_3 = cl_conv4_3.view(batch_size, -1, self.num_classes) # (N, 5776, classes)\r\n",
    "\r\n",
    "        cl_conv7 = self.cl_conv7(conv7_feat) #(N, classes*6, 19, 19)\r\n",
    "        cl_conv7 = cl_conv7.permute(0, 2, 3, 1).contiguous() # (N, 19, 19, classes*6)\r\n",
    "        cl_conv7 = cl_conv7.view(batch_size, -1, self.num_classes) # (N, 2166, classes)\r\n",
    "\r\n",
    "        cl_conv8_2 = self.cl_conv8_2(conv8_2_feat)\r\n",
    "        cl_conv8_2 = cl_conv8_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        cl_conv8_2 = cl_conv8_2.view(batch_size, -1, self.num_classes)\r\n",
    "\r\n",
    "        cl_conv9_2 = self.cl_conv9_2(conv9_2_feat)\r\n",
    "        cl_conv9_2 = cl_conv9_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        cl_conv9_2 = cl_conv9_2.view(batch_size, -1, self.num_classes)\r\n",
    "\r\n",
    "        cl_conv10_2 = self.cl_conv10_2(conv10_2_feat)\r\n",
    "        cl_conv10_2 = cl_conv10_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        cl_conv10_2 = cl_conv10_2.view(batch_size, -1, self.num_classes)\r\n",
    "\r\n",
    "        cl_conv11_2 = self.cl_conv11_2(conv11_2_feat)\r\n",
    "        cl_conv11_2 = cl_conv11_2.permute(0, 2, 3, 1).contiguous()\r\n",
    "        cl_conv11_2 = cl_conv11_2.view(batch_size, -1, self.num_classes)\r\n",
    "\r\n",
    "        \r\n",
    "        locs = torch.cat((loc_conv4_3, loc_conv7, loc_conv8_2, loc_conv9_2, loc_conv10_2, loc_conv11_2), dim=1) # dim: the dimention over which the tensors are concatnated\r\n",
    "        classifs = torch.cat((cl_conv4_3, cl_conv7, cl_conv8_2, cl_conv9_2, cl_conv10_2, cl_conv11_2), dim=1) \r\n",
    "\r\n",
    "        return locs, classifs\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def create_default_boxes():\r\n",
    "    fmap_dims = {'conv4_3': 38,\r\n",
    "                     'conv7': 19,\r\n",
    "                     'conv8_2': 10,\r\n",
    "                     'conv9_2': 5,\r\n",
    "                     'conv10_2': 3,\r\n",
    "                     'conv11_2': 1}\r\n",
    "\r\n",
    "    obj_scales = {'conv4_3': 0.1,\r\n",
    "                      'conv7': 0.2,\r\n",
    "                      'conv8_2': 0.375,\r\n",
    "                      'conv9_2': 0.55,\r\n",
    "                      'conv10_2': 0.725,\r\n",
    "                      'conv11_2': 0.9}\r\n",
    "\r\n",
    "    aspect_ratios = {'conv4_3': [1., 2., 0.5],\r\n",
    "                         'conv7': [1., 2., 3., 0.5, .333],\r\n",
    "                         'conv8_2': [1., 2., 3., 0.5, .333],\r\n",
    "                         'conv9_2': [1., 2., 3., 0.5, .333],\r\n",
    "                         'conv10_2': [1., 2., 0.5],\r\n",
    "                         'conv11_2': [1., 2., 0.5]}\r\n",
    "\r\n",
    "    fmaps = list(fmap_dims.keys())\r\n",
    "\r\n",
    "    def_boxes = []\r\n",
    "\r\n",
    "    for k, fmap in enumerate(fmaps):\r\n",
    "        for i in range(fmap_dims[fmap]):\r\n",
    "            for j in range(fmap_dims[fmap]):\r\n",
    "                cx = (j + 0.5) / fmap_dims[fmap]\r\n",
    "                cy = (i + 0.5) / fmap_dims[fmap]\r\n",
    "\r\n",
    "                for ratio in aspect_ratios[fmap]:\r\n",
    "                    def_boxes.append([cx, cy, obj_scales[fmap] * np.sqrt(ratio), obj_scales[fmap] / np.sqrt(ratio)])\r\n",
    "\r\n",
    "\r\n",
    "                    if ratio == 1.:\r\n",
    "                        try:\r\n",
    "                            additional_scale = np.sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\r\n",
    "                            \r\n",
    "                        except IndexError:\r\n",
    "                            additional_scale = 1.\r\n",
    "                        def_boxes.append([cx, cy, additional_scale, additional_scale])\r\n",
    "\r\n",
    "    def_boxes = torch.FloatTensor(def_boxes)  # (8732, 4)\r\n",
    "    def_boxes.clamp_(0, 1)  # (8732, 4)\r\n",
    "\r\n",
    "    return def_boxes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class SSD300(nn.Module):\r\n",
    "    def __init__(self, num_classes, aspect_ratios, features_size):\r\n",
    "        super(SSD300, self).__init__()\r\n",
    "\r\n",
    "        self.num_classes = num_classes\r\n",
    "\r\n",
    "        self.base = Base()\r\n",
    "        for param in self.base.parameters():\r\n",
    "            param.requires_grad = False\r\n",
    "            \r\n",
    "        self.extras = Extras()\r\n",
    "        self.predict = Predict(num_classes)\r\n",
    "\r\n",
    "        self.def_boxes = create_default_boxes()\r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, image):\r\n",
    "        conv4_3_feat, conv7_feat = self.base(image) #(N, 512, 38, 38), (N, 1024, 19, 19)\r\n",
    "\r\n",
    "        # L2 Norm \r\n",
    "        norm = conv4_3_feat.pow(2).sum(dim=1, keepdim=True) #(N, 1, 38, 38)\r\n",
    "        norm = torch.sqrt(norm)\r\n",
    "        conv4_3_feat = conv4_3_feat / norm #(N, 1, 38, 38)\r\n",
    "        conv4_3_feat = conv4_3_feat * 20 \r\n",
    "\r\n",
    "\r\n",
    "        conv8_2_feat, conv9_2_feat, conv10_2_feat, conv11_2_feat = self.extras(conv7_feat)\r\n",
    "        locs, confs = self.predict(conv4_3_feat, conv7_feat, conv8_2_feat, conv9_2_feat, conv10_2_feat, conv11_2_feat)\r\n",
    "\r\n",
    "\r\n",
    "        output = (locs, confs, self.def_boxes)\r\n",
    "        return output\r\n",
    "\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def nms(boxes, scores, overlap=0.45, top_k=200):\r\n",
    "    \"\"\" boxes: (8732, 4)\r\n",
    "        scores: (8732, )\r\n",
    "        Giu 200 box co score cao nhat\r\n",
    "        Cac box co overlap > 0.45 so voi box co score cao nhat se bi loai\r\n",
    "\r\n",
    "        Return: tensor chua index của các box thỏa mãn\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    count = 0\r\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\r\n",
    "    # keep = []\r\n",
    "    \r\n",
    "    xmin = boxes[:, 0] # (8732, )\r\n",
    "    ymin = boxes[:, 1]\r\n",
    "    xmax = boxes[:, 2]\r\n",
    "    ymax = boxes[:, 3]\r\n",
    "\r\n",
    "    area = (xmax - xmin) * (ymax - ymin)\r\n",
    "\r\n",
    "    _, idx = torch.sort(scores, dim=0, )\r\n",
    "\r\n",
    "    idx = idx[-top_k:]\r\n",
    "    \r\n",
    "\r\n",
    "    while(idx.numel() > 0):\r\n",
    "\r\n",
    "        i = idx[-1] #Lấy index của box có score cao nhất\r\n",
    "        keep[count] = i\r\n",
    "        count += 1\r\n",
    "        # keep.append(i)\r\n",
    "\r\n",
    "        if idx.size(0) == 1:\r\n",
    "            break\r\n",
    "\r\n",
    "        idx = idx[:-1]\r\n",
    "\r\n",
    "        temp_xmin = torch.index_select(input=xmin, dim=0, index=idx) # (199,)\r\n",
    "        temp_ymin = torch.index_select(input=ymin, dim=0, index=idx)\r\n",
    "        temp_xmax = torch.index_select(input=xmax, dim=0, index=idx)\r\n",
    "        temp_ymax = torch.index_select(input=ymax, dim=0, index=idx)\r\n",
    "\r\n",
    "        temp_xmin.clamp_(min=xmin[i]) #(199, )\r\n",
    "        temp_ymin.clamp_(min=ymin[i])\r\n",
    "        temp_xmax.clamp_(max=xmax[i])\r\n",
    "        temp_ymax.clamp_(max=ymax[i])\r\n",
    "        \r\n",
    "        temp_w = temp_xmax - temp_xmin #(199, )\r\n",
    "        temp_h = temp_ymax - temp_ymin\r\n",
    "\r\n",
    "        temp_w.clamp_(min=0.) # (199, )\r\n",
    "        temp_h.clamp_(min=0.)\r\n",
    "\r\n",
    "        intersect = temp_w * temp_h  #(199, )\r\n",
    "        others_area = torch.index_select(input=area, dim=0, index=idx) # (199, )\r\n",
    "\r\n",
    "        union = area[i] + others_area - 2 * intersect\r\n",
    "\r\n",
    "        iou = intersect / union \r\n",
    "\r\n",
    "        idx = idx[torch.le(iou, overlap)]\r\n",
    "\r\n",
    "    # keep = torch.tensor(keep)\r\n",
    "        \r\n",
    "    return keep, count\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "boxes = np.random.randint(1, 300, (8732,4))\r\n",
    "boxes = torch.from_numpy(boxes)\r\n",
    "\r\n",
    "scores = torch.rand(8732)\r\n",
    "\r\n",
    "keep, count = nms(boxes, scores)\r\n",
    "\r\n",
    "print(count)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "178\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def decode(locs, def_boxes):\r\n",
    "    \"\"\" Tinhs bouding boxes\r\n",
    "        locs: (8732, 4)          offsets ---> center form ---> xymin xymax form\r\n",
    "        def_boxes: (8732, 4)\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    boxes = torch.cat((\r\n",
    "        def_boxes[:, :2] + locs[:, :2] * def_boxes[:, 2:] * 0.1, #cx, cy\r\n",
    "        def_boxes[:, 2:] * torch.exp(locs[:, 2:] * 0.2)), dim=1) #w, h\r\n",
    "\r\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2  #xmin, ymin\r\n",
    "    boxes[:, 2:] += boxes[:, :2]  # xmax, ymax\r\n",
    " \r\n",
    "    return boxes\r\n",
    "\r\n",
    "def encode(matches, def_boxes):\r\n",
    "    \"\"\" Biến truth box của từng def_box về dạng offset cho mô hình\r\n",
    "        Args:\r\n",
    "            matches: (8732, 4)      xymin xymax form ---> center form ---> offset\r\n",
    "            def_boxes: (8732, 4)\r\n",
    "        Return:\r\n",
    "            offset: (8732, 4) \r\n",
    "    \"\"\"\r\n",
    "    g_cxcy = (matches[:, :2] + matches[:, 2:]) / 2 #cxcy\r\n",
    "    g_wh = matches[:, 2:] - matches[:, :2]\r\n",
    "    \r\n",
    "    g_hat_cxcy = (g_cxcy - def_boxes[:, :2]) / (def_boxes[:, 2:] * 0.1)\r\n",
    "    g_hat_wh = torch.log(g_wh / def_boxes[:, 2:]) / 0.2 \r\n",
    "\r\n",
    "    locs = torch.cat([g_hat_cxcy, g_hat_wh], dim=1)\r\n",
    "    return locs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "a = torch.rand((2, 4))\r\n",
    "b = torch.rand((2, 4))\r\n",
    "print(a)\r\n",
    "print(b)\r\n",
    "\r\n",
    "c = decode(a, b)\r\n",
    "print(c)\r\n",
    "\r\n",
    "d = encode(c, b)\r\n",
    "print(d)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0275, 0.6654, 0.1284, 0.6080],\n",
      "        [0.3845, 0.3446, 0.3520, 0.0682]])\n",
      "tensor([[0.2325, 0.9939, 0.7087, 0.2433],\n",
      "        [0.5912, 0.7395, 0.6658, 0.1927]])\n",
      "tensor([[-0.1292,  0.8727,  0.5980,  1.1475],\n",
      "        [ 0.2597,  0.6484,  0.9740,  0.8438]])\n",
      "tensor([[0.0275, 0.6654, 0.1284, 0.6080],\n",
      "        [0.3845, 0.3446, 0.3520, 0.0682]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class Detect():\r\n",
    "    def __init__(self, conf_thresh=0.01, top_k=200, nms_thresh=0.45):\r\n",
    "        self.softmax = nn.Softmax(dim=-1)\r\n",
    "        self.conf_thresh = conf_thresh\r\n",
    "        self.top_k = top_k\r\n",
    "        self.nms_thresh = nms_thresh\r\n",
    "\r\n",
    "    def forward(self, locs, confs, def_boxes):\r\n",
    "        batch_size = confs.size(0)\r\n",
    "        num_bbox = confs.size(1)\r\n",
    "        num_class = confs.size(2)\r\n",
    "\r\n",
    "        confs = self.softmax(confs) #(batch_size, 8732, num_class)\r\n",
    "        # print('confs = ', confs.shape)\r\n",
    "        confs_pred = confs.permute(0, 2, 1).contiguous() #(batch_size, num_class, 8732)\r\n",
    "        # print(confs_pred.shape)\r\n",
    "\r\n",
    "        output = torch.zeros(batch_size, num_class, self.top_k, 5)\r\n",
    "\r\n",
    "        # Xuwr lys tuwngf anhr\r\n",
    "        for i in range(batch_size):\r\n",
    "            decode_boxes = decode(locs[i], def_boxes) # (8732, 4)\r\n",
    "\r\n",
    "            confs_score = confs_pred[i].clone().detach() # (num_class, 8732)\r\n",
    "\r\n",
    "            for cl in range(1, num_class):  # Bỏ background\r\n",
    "                c_mask = confs_score[cl].gt(self.conf_thresh) # laays nhuwngx thawngf lonws hown 0.01 \r\n",
    "                scores = confs_score[cl][c_mask] #list\r\n",
    "                # print(c_mask.shape)\r\n",
    "                # print(scores.shape)\r\n",
    "                if scores.numel() == 0:\r\n",
    "                    continue\r\n",
    "                \r\n",
    "                l_mask = c_mask.unsqueeze(1).expand_as(decode_boxes) # (8732, 4)\r\n",
    "                # print(l_mask.shape)\r\n",
    "                boxes = decode_boxes[l_mask].view(-1, 4) # (abc, 4)\r\n",
    "                # print(boxes.shape)\r\n",
    "                ids, count = nms(boxes, scores, overlap=self.nms_thresh, top_k=self.top_k)\r\n",
    "\r\n",
    "                # count = len(ids)\r\n",
    "                output[i, cl, :count] = torch.cat((scores[ids[:count]].unsqueeze(1), boxes[ids[:count]]), 1)\r\n",
    "            #     print('---'*20)\r\n",
    "            # print('---'*20)\r\n",
    "        return output\r\n",
    "                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def intersect(box_a, box_b):\r\n",
    "    \"\"\"Args:\r\n",
    "        box_a : tensor (num_boxes_a, 4)\r\n",
    "        box_b : tensor (num_boxes_b, 4)\r\n",
    "\r\n",
    "       Return:\r\n",
    "        intersection area: tensor (num_boxes_A, num_boxes_B)\r\n",
    "    \"\"\"\r\n",
    "    A = box_a.size(0)\r\n",
    "    B = box_b.size(0)\r\n",
    "\r\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2), box_b[:, :2].unsqueeze(0).expand(A, B, 2))\r\n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2), box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\r\n",
    "\r\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\r\n",
    "    \r\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def jaccard(box_a, box_b):\r\n",
    "    \"\"\"\r\n",
    "    \"\"\" \r\n",
    "    inter = intersect(box_a, box_b) # (num_boxes_a, num_boxes_b)\r\n",
    "    area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]) #(num_boxes_a, )\r\n",
    "    area_b = (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]) #(num_boxes_b, )\r\n",
    "\r\n",
    "    area_a.unsqueeze_(1).expand_as(inter)\r\n",
    "    area_b.unsqueeze_(0).expand_as(inter)\r\n",
    "\r\n",
    "    union = area_a + area_b - inter\r\n",
    "\r\n",
    "    return inter / union"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def cxcy_to_xy(boxes):\r\n",
    "    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\r\n",
    "    representation for comparison to point form ground truth data.\r\n",
    "    Args:\r\n",
    "        boxes: (tensor) center-size default boxes from priorbox layers.\r\n",
    "    Return:\r\n",
    "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\r\n",
    "    \"\"\"\r\n",
    "    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\r\n",
    "                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def match(threshhold, truths, def_boxes, labels, locs_t, confs_t, idx):\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "        Match each prior box with the ground truth box of the highest jaccard\r\n",
    "    overlap, encode the bounding boxes, then return the matched indices\r\n",
    "    corresponding to both confidence and location preds.\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    dbox_xy = cxcy_to_xy(def_boxes) # to xmin ymin xmax ymax (8732, 4)\r\n",
    "\r\n",
    "    overlap = jaccard(truths, dbox_xy) # (num_truth_boxes, 8732)\r\n",
    "\r\n",
    "    best_dbox_overlap, best_dbox_idx = torch.max(overlap, dim=1) # (num_truth_boxes,)\r\n",
    "    best_truth_overlap, best_truth_idx = torch.max(overlap, dim=0) #(8732,)\r\n",
    "\r\n",
    "    best_truth_overlap.index_fill_(0, best_dbox_idx, 2)  # to ensure best dbox\r\n",
    "\r\n",
    "    for j in range(best_dbox_idx.size(0)):\r\n",
    "        best_truth_idx[best_dbox_idx[j]] = j\r\n",
    "\r\n",
    "    matches = truths[best_truth_idx]  # (8732, 4)\r\n",
    "    confs = labels[best_truth_idx] + 1 # set label from truth box to each dbox (8732,)\r\n",
    "    confs[best_truth_overlap < threshhold] = 0 # set background to 0 #(8732, )\r\n",
    "\r\n",
    "    print(truths.shape)\r\n",
    "    print(matches.shape)\r\n",
    "\r\n",
    "    locs = encode(matches, def_boxes)  # (8732, 4)\r\n",
    "    locs_t[idx] = locs \r\n",
    "    confs_t[idx] = confs\r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "truths = torch.rand(3, 4)\r\n",
    "truths"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.5345, 0.3402, 0.4125, 0.6527],\n",
       "        [0.3040, 0.2224, 0.5577, 0.6542],\n",
       "        [0.9489, 0.4620, 0.1342, 0.4845]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class MultiboxLoss(nn.Module):\r\n",
    "    \"\"\" \r\n",
    "        1. localization loss\r\n",
    "        2. confidence loss for predict class score\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, threshold=0.5, neg_pos_ratio=3, alpha=1):\r\n",
    "        super(MultiboxLoss, self).__init__()\r\n",
    "\r\n",
    "        self.threshold = threshold\r\n",
    "        self.neg_pos_ratio = neg_pos_ratio\r\n",
    "        self.alpha = alpha\r\n",
    "        self.L1Loss = nn.SmoothL1Loss(reduction='sum')\r\n",
    "        self.CrossentropyLoss = nn.CrossEntropyLoss(reduction='none')\r\n",
    "\r\n",
    "    def forward(self, predictions, targets):\r\n",
    "        locs, confs, def_boxes = predictions # (batch, 8732, 4), (batch, 8732, num_classes), (8732, 4)\r\n",
    "\r\n",
    "        batch_size = confs.size(0)\r\n",
    "        num_boxes = confs.size(1)\r\n",
    "        num_classes = confs.size(2)\r\n",
    "\r\n",
    "        confs_t_labels = torch.LongTensor(batch_size, num_boxes)\r\n",
    "        locs_t = torch.Tensor(batch_size, num_boxes, 4)\r\n",
    "\r\n",
    "        for i in range(batch_size):\r\n",
    "            truths = targets[i][:, :-1] # xmin, ymin, xmax, ymax\r\n",
    "            labels = targets[i][:, -1] # label\r\n",
    "            match(self.threshold, truths, def_boxes, labels, locs_t, confs_t_labels, i)\r\n",
    "\r\n",
    "    #Loc_loss\r\n",
    "        pos_mask = confs_t_labels > 0  #(batch, 8732)\r\n",
    "        pos_idx = pos_mask.unsqueeze(2).expand_as(locs_t) #(batch, 8732, 4)\r\n",
    "\r\n",
    "        locs_p = locs[pos_idx]\r\n",
    "        locs_t = locs_t[pos_idx]\r\n",
    "\r\n",
    "        # print(pos_mask.shape)\r\n",
    "        # print(locs.shape)\r\n",
    "        # print(locs_p.shape)\r\n",
    "        # print(locs_t.shape)\r\n",
    "        \r\n",
    "        loc_loss = self.L1Loss(locs_p, locs_t)\r\n",
    "        # print(loc_loss)\r\n",
    "\r\n",
    "    # Conf_loss\r\n",
    "        true_classes = confs_t_labels.view(-1) #(8732)\r\n",
    "        predicted_scores = confs.view(-1, num_classes)\r\n",
    "\r\n",
    "        conf_loss_all = self.CrossentropyLoss(predicted_scores, true_classes) # (batch * 8732)\r\n",
    "        conf_loss_all = conf_loss_all.view(batch_size, -1) # (batch, 8732)\r\n",
    "\r\n",
    "        # hard negative mining\r\n",
    "        n_positive = pos_mask.sum(dim=1) # (batch,)\r\n",
    "        # print(n_positive.shape)\r\n",
    "        n_hard_negative = torch.clamp(n_positive*self.neg_pos_ratio, max=num_boxes) # (batch, )\r\n",
    "\r\n",
    "        conf_loss_pos = conf_loss_all[pos_mask] #(N, )\r\n",
    "\r\n",
    "        conf_loss_neg = conf_loss_all.clone()\r\n",
    "        conf_loss_neg[pos_mask] = 0.  # chuyển loss của positive về 0\r\n",
    "\r\n",
    "        conf_loss_neg, _ = torch.sort(conf_loss_neg, dim=1, descending=True)\r\n",
    "\r\n",
    "        loss_rank = torch.LongTensor(range(num_boxes)).unsqueeze(0).expand_as(conf_loss_neg) # (batch, 8732)\r\n",
    "        hard_negative = loss_rank < n_hard_negative.unsqueeze(1).expand_as(loss_rank)\r\n",
    "\r\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negative] \r\n",
    "\r\n",
    "        conf_loss = (conf_loss_pos.sum() + conf_loss_hard_neg.sum()) \r\n",
    "        # print(conf_loss)\r\n",
    "\r\n",
    "        loss = (conf_loss + self.alpha * loc_loss) / n_positive.sum().float()\r\n",
    "        loss.nan_to_num_(nan=0) # set NaN value to 0\r\n",
    "\r\n",
    "        return loss\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def train(train_loader, model, criterion, optimizer, epochs):\r\n",
    "    model.to(device)\r\n",
    "\r\n",
    "\r\n",
    "    for epoch in epochs:\r\n",
    "        epoch_loss = 0.0\r\n",
    "        epoch_start = time.time()\r\n",
    "        iter_start = time.time()\r\n",
    "        print(\"---\" * 20)\r\n",
    "        print('Epoch {} / {}:'.format(epoch+1, epoch))\r\n",
    "\r\n",
    "        model.train()\r\n",
    "\r\n",
    "        for i, (images, targets) in enumerate(train_loader):\r\n",
    "\r\n",
    "            images = images.to(device)\r\n",
    "            targets = targets.to(device)\r\n",
    "\r\n",
    "            outputs = model(images)\r\n",
    "            loss = criterion(outputs, target)\r\n",
    "\r\n",
    "            optimizer.zero_grad()\r\n",
    "            loss.backward()\r\n",
    "\r\n",
    "            nn.utils.clip_grad_value_(model.parameters(), clip_value=2.0)\r\n",
    "            optimizer.step()\r\n",
    "\r\n",
    "            if i % print_freq == 0:\r\n",
    "                iter_finish = time.time()\r\n",
    "                time_per_iter = inter_finish - iter_start\r\n",
    "                print('\\t\\t Time for iter: {} s, loss: {:4f}'.format(time_per_iter, loss.item()))\r\n",
    "                iter_start = time.time()\r\n",
    "\r\n",
    "            epoch_loss += loss.item() \r\n",
    "        epoch_loss = epoch_loss / (i + 1)\r\n",
    "\r\n",
    "        epoch_finish = time.time()\r\n",
    "        time_per_epoch = epoch_finish - epoch_start\r\n",
    "        print('\\t Time for epoch: {} s, loss: {}'.format(time_per_epoch, epoch_loss))\r\n",
    "\r\n",
    "        if (epoch + 1) % 10 == 0:\r\n",
    "            torch.save(model.state_dict(), './weights/ssd300_' + str(epoch + 1) + '.pth') \r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "class VOC2007Detection(Dataset):\r\n",
    "    def __init__(self, root,classes, transform=None, image_set='trainval', transform=None):\r\n",
    "        self.root = root\r\n",
    "        self.classes = classes\r\n",
    "        self.transform = transform\r\n",
    "        self.image_set = image_set  # trainval or test\r\n",
    "\r\n",
    "        self.anno_path = os.path.join(self.root, 'Annotations/%s.xml')\r\n",
    "        self.img_path = os.path.join(self.root, 'JPEGImages/%s.jpg')\r\n",
    "        self.ids = []\r\n",
    "\r\n",
    "        train_id_path = os.path.join(root, 'ImageSets/Main/train.txt')\r\n",
    "        val_id_path = os.path.join(root, 'ImageSets/Main/val.txt')\r\n",
    "        \r\n",
    "        with open(train_id_path, 'r') as f:\r\n",
    "            for line in f:\r\n",
    "                self.ids.append(line.strip())\r\n",
    "        with open(val_id_path, 'r') as f:\r\n",
    "            for line in f:\r\n",
    "                self.ids.append(line.strip())\r\n",
    "        \r\n",
    " \r\n",
    "    def __getitem__(self, index):\r\n",
    "        targets = self.get_annotation(index)\r\n",
    "        images = self.get_image(index)\r\n",
    "\r\n",
    "        return images, targets\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.ids)\r\n",
    "\r\n",
    "        \r\n",
    "\r\n",
    "    def get_annotation(self, index):\r\n",
    "        targets = []\r\n",
    "        path = self.anno_path % self.ids[index]\r\n",
    "        xml = ET.parse(path).getroot()\r\n",
    "\r\n",
    "        for item in xml.iter('size'):\r\n",
    "            width = int(item.find('width').text)\r\n",
    "            height = int(item.find('height').text)\r\n",
    "            channel = int(item.find('depth').text)\r\n",
    "        \r\n",
    "        for obj in xml.iter('object'):\r\n",
    "            difficult = obj.find('difficult').text\r\n",
    "            if difficult == 1:\r\n",
    "                continue\r\n",
    "            \r\n",
    "            name = obj.find('name').text.lower().strip()\r\n",
    "            bbox = obj.find('bndbox')\r\n",
    "        \r\n",
    "            bndbox = []\r\n",
    "            points = ['xmin', 'ymin', 'xmax', 'ymax']\r\n",
    "\r\n",
    "            for item in points:\r\n",
    "                point = int(bbox.find(item).text) - 1\r\n",
    "                bndbox.append(point)\r\n",
    "\r\n",
    "            label_id = self.classes.index(name)\r\n",
    "            bndbox.append(label_id)  #(xmin, ymin, xmax, ymax, label)\r\n",
    "\r\n",
    "            targets.append(bndbox)\r\n",
    "        targets = torch.tensor(targets, dtype=torch.float)\r\n",
    "\r\n",
    "        return targets\r\n",
    "    \r\n",
    "    def get_image(self, index):\r\n",
    "        path = self.img_path % self.ids[index]\r\n",
    "        image = cv2.imread(path) #(BGR)\r\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n",
    "        \r\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\r\n",
    "\r\n",
    "        return image\r\n",
    "\r\n",
    "    def collate_fn(self, batch):\r\n",
    "        images = []\r\n",
    "        targets = []\r\n",
    "\r\n",
    "        for item in batch:\r\n",
    "            images.append(item[0])\r\n",
    "            targets.append(item[1])\r\n",
    "       \r\n",
    "        images = torch.stack(images, dim=0) #(batch_size, 3, 300, 300)\r\n",
    "\r\n",
    "        return images, targets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "class NormalizeCoords(object):\r\n",
    "    def __call__(self, image, boxes, labels=None):\r\n",
    "        height, weight, channel = image.shape\r\n",
    "\r\n",
    "        boxes[:, 0] /= weight\r\n",
    "        boxes[:, 1] /= height\r\n",
    "        boxes[:, 2] /= weight\r\n",
    "        boxes[:, 3] /= height\r\n",
    "        \r\n",
    "        return image, boxes, labels\r\n",
    "\r\n",
    "\r\n",
    "class Resize(object):\r\n",
    "    def __init__(self, size):\r\n",
    "        self.size = size\r\n",
    "    def __call__(self, image, boxes, labels=None):\r\n",
    "        old_h, old_w, channel = image.shape\r\n",
    "\r\n",
    "        padd_top = max((old_w - old_h) // 2, 0)\r\n",
    "        padd_left = max((old_h - old_w) // 2, 0)\r\n",
    "\r\n",
    "        image = cv2.copyMakeBorder(image, padd_top, padd_top, padd_left, padd_left, cv2.BORDER_CONSTANT, (0, 0, 0))\r\n",
    "\r\n",
    "        image = cv2.resize(image, (self.size, self.size))\r\n",
    "\r\n",
    "        boxes[:, 0] = (boxes[:, 0] + padd_left) * self.size / max(old_h, old_w)\r\n",
    "        boxes[:, 1] = (boxes[:, 1] + padd_top) * self.size / max(old_h, old_w)\r\n",
    "        boxes[:, 2] = (boxes[:, 2] + padd_left) * self.size / max(old_h, old_w)\r\n",
    "        boxes[:, 3] = (boxes[:, 3] + padd_top) * self.size / max(old_h, old_w)\r\n",
    "        \r\n",
    "        return image, boxes, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "classes = [\"aeroplane\", \"bicycle\", \"bird\",  \"boat\", \"bottle\", \r\n",
    "               \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\r\n",
    "               \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\",\r\n",
    "               \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\r\n",
    "               \r\n",
    "dataset = VOC2007Detection(root='G:/VOC 2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/', classes=classes)\r\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=16, shuffle=True, collate_fn=dataset.collate_fn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "for image, target in iter(dataloader):\r\n",
    "    print(image.shape)\r\n",
    "    print(target)\r\n",
    "    break\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([16, 3, 300, 300])\n",
      "[tensor([[ 56, 167, 200, 243,   2],\n",
      "        [267, 102, 445, 234,   2]]), tensor([[103, 109, 417, 211,   6],\n",
      "        [230, 122, 273, 147,  14]]), tensor([[ 32, 232, 159, 384,   8]]), tensor([[ 89,  25, 322, 329,  16]]), tensor([[130,   1, 498, 348,  18],\n",
      "        [343,  98, 365, 118,  14],\n",
      "        [208,  88, 257, 230,  14]]), tensor([[ 75, 116, 156, 305,  14],\n",
      "        [305, 114, 373, 247,  14],\n",
      "        [346, 114, 381, 211,  14],\n",
      "        [372,  74, 461, 374,  14],\n",
      "        [167, 130, 354, 374,  14]]), tensor([[283, 377, 334, 499,   8],\n",
      "        [ 28,  75, 253, 499,  14],\n",
      "        [  1, 244, 116, 452,  14]]), tensor([[195, 128, 396, 314,   2]]), tensor([[125, 160, 315, 267,  17],\n",
      "        [293, 150, 335, 189,  17],\n",
      "        [333, 148, 387, 202,   8]]), tensor([[159, 262, 317, 374,  14],\n",
      "        [331, 244, 438, 374,  14],\n",
      "        [414, 216, 478, 360,  14],\n",
      "        [396, 141, 499, 346,  14],\n",
      "        [255, 101, 382, 268,  14],\n",
      "        [180, 132, 256, 222,  14],\n",
      "        [105, 134, 139, 185,  14],\n",
      "        [  0,  92, 170, 374,  14],\n",
      "        [ 94, 222, 416, 374,  10]]), tensor([[127, 180, 273, 298,   6],\n",
      "        [442, 124, 474, 209,  14],\n",
      "        [349, 157, 444, 187,   6]]), tensor([[417, 152, 497, 219,  19],\n",
      "        [ 18, 278, 174, 374,   8],\n",
      "        [ 13, 237, 116, 314,   8]]), tensor([[185, 150, 376, 353,  14],\n",
      "        [125, 174, 328, 353,   8]]), tensor([[ 50, 128, 351, 267,   5],\n",
      "        [412, 203, 426, 233,  14],\n",
      "        [ 90, 213, 137, 280,  14],\n",
      "        [ 57, 234, 159, 300,  13]]), tensor([[104,  31, 408, 320,  19]]), tensor([[  8,  99, 322, 230,   6]])]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "path = 'G:/VOC 2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/ImageSets/Main/trainval.txt'\r\n",
    "temp = []\r\n",
    "with open(path, 'r') as f:\r\n",
    "    for line in f:\r\n",
    "        temp.append(line.strip())\r\n",
    "\r\n",
    "len(temp)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5011"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "x = cv2.imread('000007.jpg')\r\n",
    "x = cv2.resize(x, (300, 300))\r\n",
    "x = torch.from_numpy(x)\r\n",
    "x.unsqueeze_(0)\r\n",
    "x = x.reshape(1, 3, 300, 300) / 255.\r\n",
    "\r\n",
    "aspect_ratios = [[1, 2, 0.5], [1, 2, 3, 0.5, 0.333], [1, 2, 3, 0.5, 0.333], [1, 2, 3, 0.5, 0.333], [1, 2, 0.5], [1, 2, 0.5]]\r\n",
    "feature_size = [38, 19, 10, 5, 3, 1]\r\n",
    "\r\n",
    "\r\n",
    "def_boxes = create_default_boxes()\r\n",
    "\r\n",
    "model = SSD300(21, aspect_ratios, feature_size)\r\n",
    "# print(model)\r\n",
    "print('Trainable params =',sum(p.numel() for p in model.parameters() if p.requires_grad))\r\n",
    "print('Total params =',sum(p.numel() for p in model.parameters()))\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained model VGG to Base.\n",
      "Trainable params = 5570670\n",
      "Total params = 26054574\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "criterion = MultiboxLoss()\r\n",
    "\r\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "a = torch.rand(1, 3, 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "for i in range(1):\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    outputs = model(x)\r\n",
    "    loss = criterion(predictions=outputs, targets=a)\r\n",
    "    optim.zero_grad()\r\n",
    "    loss.backward()\r\n",
    "    optim.step()\r\n",
    "print(loss.item())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\dangv\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([8732, 4])\n",
      "0.0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "bda2dbd333357889da984cf9be9c50cb5e8115445fa270a04925ab0de91782a8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}